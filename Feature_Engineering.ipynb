{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Engineering**\n",
        "\n",
        "Question 1: What is a parameter?\n",
        "\n",
        "Answer: A parameter typically refers to a configurable value or setting used in the process of transforming raw data into features suitable for machine learning models. These parameters influence how features are created or processed, but they are not learned from the data like model parameters\n",
        "\n",
        "Question 2: What is correlation? What does negative correlation mean?\n",
        "\n",
        "Answer: Correlation refers to the statistical relationship between two features. It is usually to understand how one feature varies with another, particularly in relation to the target variable.\n",
        "\n",
        "A negative correlation means that as one variable increases, the other decreases — they move in opposite directions.\n",
        "\n",
        "Question 3: Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "Answer: Machine Learning is a branch of artificial intelligence (AI) that enables computers to learn from data and make decisions or predictions without being explicitly programmed for every scenario.\n",
        "\n",
        "> Main Components of Machine Learning: To build and use machine learning systems effectively, several core components are involved:\n",
        "\n",
        "1. Data\n",
        "- The foundation of machine learning.\n",
        "- Comes in the form of:\n",
        "    - Features: Input variables (e.g., age, temperature, text).\n",
        "    - Labels/Targets: Desired output (used in supervised learning).\n",
        "- Types: Structured (tables), unstructured (images, audio, text).\n",
        "\n",
        "2. Model\n",
        "- A mathematical representation that maps inputs (features) to outputs (predictions).\n",
        "- Examples:\n",
        "    - Linear regression, decision trees, neural networks, etc.\n",
        "\n",
        "3. Algorithm\n",
        "- The procedure used to train the model using data.\n",
        "- It adjusts model parameters to minimize error.\n",
        "- Examples:\n",
        "    - Gradient descent (for optimization),\n",
        "    - k-Nearest Neighbors (for classification),\n",
        "    - ID3 (for decision trees).\n",
        "\n",
        "4. Training\n",
        "- The process of feeding data into a model so it can learn patterns.\n",
        "- Involves:\n",
        "    - Optimizing weights/parameters.\n",
        "    - Minimizing a loss function.\n",
        "\n",
        "5. Evaluation\n",
        "- Measuring the model’s performance using metrics on unseen (test or validation) data.\n",
        "- Common metrics:\n",
        "    - Accuracy, Precision, Recall, F1-score (classification)\n",
        "    - MSE, RMSE, MAE (regression)\n",
        "\n",
        "6. Prediction/Inference\n",
        "- Once trained, the model is used to make predictions or decisions on new, unseen data.\n",
        "\n",
        "7. Feedback Loop (Optional)\n",
        "- In some systems (like online recommendation engines), model predictions are updated continuously using new data (a concept in online learning).\n",
        "\n",
        "Question 4: How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "Answer: The loss value is a quantitative measure of how far off a machine learning model’s predictions are from the actual values (ground truth). It plays a central role in training and evaluating a model. The loss value helps determine how well a model is performing by measuring how far its predictions are from the actual values. A lower loss typically means a better model, but it must be interpreted in context with validation data and other metrics like accuracy or F1-score.\n",
        "\n",
        "Question 5: What are continuous and categorical variables?\n",
        "\n",
        "Answer:\n",
        "1. Continuous Variables: A continuous variable is a numerical variable that can take any value within a range — including decimals.\n",
        "\n",
        "    ✅ Characteristics:\n",
        "    - Infinite possible values within a range.\n",
        "    - Values are measurable (not counted).\n",
        "    - You can perform arithmetic operations (add, subtract, average, etc.).\n",
        "\n",
        "2. Categorical Variables: A categorical variable (also called a qualitative variable) represents categories or groups. These values are labels and not numerical (even if they use numbers).\n",
        "\n",
        "    ✅ Characteristics:\n",
        "    - Values fall into a fixed number of groups.\n",
        "    - Usually not suitable for arithmetic operations.\n",
        "    - Can be ordinal (ordered) or nominal (unordered)\n",
        "\n",
        "Question 6: How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "Answer: Machine learning models require numerical input, but categorical variables are non-numeric labels. So, before feeding them into a model, you need to convert them into numeric form using encoding techniques.\n",
        "\n",
        "Common Techniques to Handle Categorical Variables:\n",
        "1. Label Encoding: Assigns a unique integer to each category.\n",
        "2. One-Hot Encoding: Creates binary columns for each category.\n",
        "3. Ordinal Encoding: Similar to label encoding, but used when category order matters.\n",
        "4. Binary Encoding: Combines label encoding and binary representation.\n",
        "5. Target Encoding (Mean Encoding): Replaces categories with the mean of the target variable for that category.\n",
        "6. Frequency / Count Encoding: Replaces each category with its frequency or count in the dataset.\n",
        "\n",
        "Question 7: What do you mean by training and testing a dataset?\n",
        "\n",
        "Answer: In machine learning, the dataset is typically split into separate parts to build and evaluate a model:\n",
        "1. Training Dataset: The training dataset is the portion of data used to train the machine learning model — that is, to teach the model the patterns in the data.\n",
        "\n",
        "2. Testing Dataset: The testing dataset is a separate portion of data used after training to evaluate how well the model performs on unseen data.\n",
        "\n",
        "Question 8: What is sklearn.preprocessing?\n",
        "\n",
        "Answer: sklearn.preprocessing is a module in Scikit-learn (sklearn) that provides tools for data preprocessing and feature transformation. These tools help prepare your data before feeding it into a machine learning model.\n",
        "\n",
        "Question 9: What is a Test set?\n",
        "\n",
        "Answer: A test set is a subset of data used to evaluate the performance of a machine learning model after it has been trained.\n",
        "\n",
        "Question 10: How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
        "\n",
        "Answer: The most common way is to use train_test_split from scikit-learn."
      ],
      "metadata": {
        "id": "NUjHQ8P1RzFR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqR-hRMIRtmd",
        "outputId": "3ead870f-c75e-4111-a5f2-811bdde93c48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: (120, 4)\n",
            "Test set size: (30, 4)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training set size:\", X_train.shape)\n",
        "print(\"Test set size:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ General Approach to a Machine Learning Problem: Here's a high-level workflow I usually follow:\n",
        "\n",
        "Step 1: Understand the problem\n",
        "- What is the task? Classification, regression, clustering?\n",
        "- What are the evaluation metrics? Accuracy, F1-score, RMSE, etc.?\n",
        "\n",
        "Step 2: Collect and explore the data\n",
        "- Load the dataset.\n",
        "- Analyze features, check for missing values, data types.\n",
        "- Visualize distributions and relationships.\n",
        "\n",
        "Step 3: Preprocess the data\n",
        "- Handle missing values.\n",
        "- Encode categorical variables.\n",
        "- Normalize or scale features if needed.\n",
        "\n",
        "Step 4: Split the data\n",
        "- Divide into training and testing sets.\n",
        "- Sometimes also create a validation set or use cross-validation.\n",
        "\n",
        "Step 5: Select and train models\n",
        "- Choose algorithms appropriate for the problem.\n",
        "- Train models on the training set.\n",
        "\n",
        "Step 6: Evaluate the models\n",
        "- Use the test set (or validation set) to check model performance.\n",
        "- Tune hyperparameters if needed.\n",
        "\n",
        "Step 7: Interpret and validate\n",
        "- Understand the model’s decisions.\n",
        "- Check for biases or errors.\n",
        "\n",
        "Step 8: Deploy or present results\n",
        "- Depending on your goal, deploy the model or communicate findings.\n",
        "\n",
        "Question 11: Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "Answer: Exploratory Data Analysis (EDA) is a crucial step before fitting a machine learning model because it helps you understand your data deeply. Here’s why it’s important:\n",
        "\n",
        "1. Understand the Data Distribution\n",
        "    - EDA helps you see how your features and target variables are distributed.\n",
        "    - You can spot skewness, outliers, or unusual patterns that may affect model performance.\n",
        "\n",
        "2. Identify Missing or Erroneous Data\n",
        "    - Missing values or incorrect entries can cause errors or bias in your model.\n",
        "    - EDA lets you detect these issues early so you can decide how to handle them (impute, drop, etc.).\n",
        "\n",
        "3. Detect Relationships and Correlations\n",
        "    - Understanding how features relate to each other and the target can guide feature selection.\n",
        "    - Strongly correlated or redundant features might be dropped or combined.\n",
        "\n",
        "4. Select Appropriate Features\n",
        "    - EDA helps to spot which variables might be more important.\n",
        "    - You can engineer new features or transform existing ones based on insights.\n",
        "\n",
        "5. Choose the Right Model and Preprocessing\n",
        "    - Some algorithms work better with certain data distributions or types.\n",
        "    - For example, if data is not normally distributed, you might apply transformations before training.\n",
        "\n",
        "6. Prevent Garbage In, Garbage Out\n",
        "    - If your data has problems that you don’t address, the model’s predictions won’t be reliable.\n",
        "    - EDA ensures you feed clean, meaningful data to your model.\n",
        "\n",
        "Question 12: What is correlation?\n",
        "\n",
        "Answer: Answer already given in question 2\n",
        "\n",
        "Question 13: What does negative correlation mean?\n",
        "\n",
        "Answer: Answer already given in question 2\n",
        "\n",
        "Question 14: How can you find correlation between variables in Python?\n",
        "\n",
        "Answer: Finding correlations between variables in Python is pretty straightforward, especially with libraries like Pandas and Seaborn. Here are some common ways:\n",
        "1. Using Pandas .corr(): If you have a DataFrame, you can calculate the correlation matrix easily\n",
        "2. Correlation with a target variable: If you want the correlation between each feature and the target\n",
        "3. Visualizing correlation with a heatmap: To better visualize the correlation matrix\n",
        "\n",
        "Question 15: What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "Answer: Causation means that one event directly causes another. In other words, a change in variable A produces a change in variable B.\n",
        "\n",
        "Difference between Correlation and Causation\n",
        "\n",
        "- Correlation: It is a statistical relationship or association between two variables. For example:\n",
        "\n",
        "    - Ice cream sales and drowning incidents both increase during the summer. They’re correlated (both go up), but ice cream sales don’t cause drownings. Instead, a lurking variable (hot weather) causes both.\n",
        "\n",
        "\n",
        "- Causation: One variable directly affects or causes changes in another. For Example:\n",
        "\n",
        "    - Smoking causes an increase in the risk of lung cancer. This is supported by experiments and studies showing a direct causal link.\n",
        "\n",
        "Question 16: What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "Answer: An optimizer is an algorithm or method used to adjust the parameters (weights) of a model to minimize the loss function during training.\n",
        "\n",
        "- The loss function measures how well the model is doing.\n",
        "- The optimizer updates the model’s parameters step-by-step to reduce this loss.\n",
        "- It helps the model “learn” from data by tweaking weights to improve predictions.\n",
        "\n",
        "Common Types of Optimizers\n",
        "1. Gradient Descent (GD)\n",
        "- The simplest optimizer.\n",
        "- Calculates the gradient of the loss function with respect to each parameter using the entire training dataset.\n",
        "- Updates weights in the opposite direction of the gradient to minimize loss.\n",
        "- Example:\n",
        "        \n",
        "        weights = weights - learning_rate * gradient\n",
        "    - Pros: Simple, effective.\n",
        "    - Cons: Can be slow for large datasets since it uses all data every step.\n",
        "2. Stochastic Gradient Descent (SGD)\n",
        "- Instead of using the whole dataset, updates weights using a single training example at a time.\n",
        "- Makes updates more frequent but noisier.\n",
        "- Example:\n",
        "      \n",
        "      for each training example:    \n",
        "          weights = weights - learning_rate * gradient(example)\n",
        "\n",
        "    - Pros: Faster updates, can escape shallow local minima.\n",
        "    - Cons: Noisier updates, less stable convergence.\n",
        "3. Mini-batch Gradient Descent\n",
        "- A middle ground: updates weights using small batches (subsets) of data.\n",
        "- Balances noise and speed.\n",
        "4. Momentum\n",
        "- Enhances SGD by adding a “momentum” term that accumulates past gradients to smooth updates.\n",
        "- Helps accelerate learning and avoid oscillations.\n",
        "- Update rule:\n",
        "\n",
        "      velocity = momentum * velocity - learning_rate * gradient\n",
        "      weights = weights + velocity\n",
        "5. Adam (Adaptive Moment Estimation)\n",
        "- One of the most popular optimizers today.\n",
        "- Combines ideas from Momentum and RMSProp.\n",
        "- Keeps track of an exponentially decaying average of past gradients and squared gradients.\n",
        "- Adapts learning rates for each parameter individually.\n",
        "- Example usage in Keras:\n",
        "\n",
        "      from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "      optimizer = Adam(learning_rate=0.001)\n",
        "      model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
        "\n",
        "Question 17: What is sklearn.linear_model?\n",
        "\n",
        "Answer: sklearn.linear_model is a module in the Scikit-learn library that provides various linear models for regression and classification tasks in machine learning.\n",
        "\n",
        "✅ What is Scikit-learn?\n",
        "- Scikit-learn (sklearn) is one of the most widely used machine learning libraries in Python.\n",
        "- It provides simple and efficient tools for data analysis and modeling.\n",
        "\n",
        "Question 18: What does model.fit() do? What arguments must be given?\n",
        "\n",
        "Answer: The .fit() method in scikit-learn is used to train (fit) a machine learning model on your data. Basic scikit-learn models, only X and y are typically required.\n",
        "\n",
        "Required Arguments\n",
        "\n",
        "      model.fit(X, y)\n",
        "\n",
        "X: Your features/input variables\n",
        "  - Typically a 2D array or DataFrame (n_samples × n_features)\n",
        "  - Example: size, age, color, etc.\n",
        "\n",
        "Y: Your target/output variable\n",
        "  - A 1D array or Series of labels or values.\n",
        "  - Can be:\n",
        "    - Continuous values (for regression)\n",
        "    - Class labels (for classification)\n",
        "\n",
        "Question 19: What does model.predict() do? What arguments must be given?\n",
        "\n",
        "Answer: Once your model has been trained using .fit(), the .predict() method is used to: Make predictions on new, unseen data (features only, not labels).\n",
        "\n",
        "Required Arguments\n",
        "\n",
        "      model.predict(X)\n",
        "\n",
        "\n",
        "X: A 2D array or DataFrame of input features (same shape and format as the training data).\n",
        "\n",
        "Shape: (n_samples, n_features)\n",
        "\n",
        "You must not include the target (y) here — just the features.\n",
        "\n",
        "Question 20: What are continuous and categorical variables?\n",
        "\n",
        "Answer: Answer already given in question 5.\n",
        "\n",
        "Question 21: What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "Answer: Feature scaling is the process of transforming the features (input variables) in your dataset so that they are on a similar scale (range of values).\n",
        "\n",
        "This is important because many machine learning algorithms perform better or converge faster when features are scaled properly.\n",
        "\n",
        "1. Improves Model Performance\n",
        "    - Algorithms like SVM, KNN, Logistic Regression, Neural Networks, and Gradient Descent-based models are sensitive to feature scales.\n",
        "2. Faster Convergence\n",
        "    - Feature scaling helps gradient descent converge faster during training.\n",
        "3. Equal Feature Importance\n",
        "    - Prevents features with larger ranges from dominating learning.\n",
        "4. Required for Distance-Based Models\n",
        "    - Models like KNN, K-Means, and PCA rely on calculating distances, which get distorted if scales differ.\n",
        "\n",
        "Question 22: How do we perform scaling in Python?\n",
        "\n",
        "Answer:\n",
        "\n",
        "🔹 Step 1: Import the Scaler\n",
        "- Scikit-learn provides several scaling classes.\n",
        "\n",
        "🔹 Step 2: Choose a Scaling Technique\n",
        "    \n",
        "  1. StandardScaler (Z-score normalization)\n",
        "      - Transforms data so that it has mean = 0 and standard deviation = 1.\n",
        "      - Best for: Most ML models, especially if the data is normally distributed.\n",
        "  2. MinMaxScaler (Normalization)\n",
        "      - Transforms features to a fixed range [0, 1].\n",
        "      - Best for: Algorithms that need bounded input, like neural networks.\n",
        "  3. RobustScaler (for outliers)\n",
        "      - Uses median and interquartile range (IQR).\n",
        "      - Best for: Datasets with outliers.\n",
        "\n",
        "🔹 Step 3: Fit and Transform the Data\n",
        "- fit() computes the scaling parameters (mean, std, etc.).\n",
        "- transform() applies the scaling to the data.\n",
        "- fit_transform() does both in one step.\n",
        "\n",
        "🔹 Step 4: Apply Scaling Correctly (Train/Test Split)\n",
        "- You should fit on the training set and transform both train and test:\n",
        "\n",
        "Question 23: What is sklearn.preprocessing?\n",
        "\n",
        "Answer: Answer already given in question 8\n",
        "\n",
        "Question 24: How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "Answer: Answer already given in question 10\n",
        "\n",
        "Question 25: Explain data encoding?\n",
        "\n",
        "Answer: Data encoding is the process of converting categorical (non-numeric) data into a numerical format so that it can be used in machine learning models. Most ML models can't handle text or categories directly — they need numbers to perform calculations."
      ],
      "metadata": {
        "id": "XudOWe7NZIZB"
      }
    }
  ]
}